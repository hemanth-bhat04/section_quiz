Today's session will focus on the PD flow, specifically the automatic place and route (APR) flow. This flow is an essential part of the PD process and will be covered in detail within the PD project. The API flow will also be discussed in depth for PD students. However, ATL students should understand what the PD flow entails, which is why there is one day dedicated to PD and another to STA. Later on, ATL teams will split into different roles. In today's session, we will briefly discuss the entire PD flow, also known as the APR flow. Here is a small flowchart of the ASE design flow, which you have already seen.
We have already seen this in the C-to-C class, and we will discuss it again here. In this ASIC design flowchart, we refer to this as the "grand ASIC flowchart." We also have a simplified ASIC flowchart. In the C-to-C class, you might have learned about the simplified flowchart, or you might have looked into this flow. So, we will examine this flow in detail. Just give me a second, and I'll use the notation.

This grand flowchart is divided into four main categories. The first one is called RTL design, the second part is RTL verification, the third one is synthesis, and the fourth one is physical design. This is the final part. In this physical design section, we are discussing the APR (Physical Design) flow.
So, in today's session, we will discuss the API flow. Under the "Physical Design" subtopic, we will focus on the API flow. Before that, let's briefly touch upon the three main concepts: RTL design, RTL verification, and synthesis. Starting with RTL design, the entire flow can be divided into four main stages. In RTL design, the first stage is MRD (Market Research Development). MRD involves studying market requirements. Each product company has an MRD team consisting of members who analyze market needs. They assess future market requirements for the next one, two, or three years. Based on this analysis, they develop specifications for the market requirements. From this market requirement document...
From the Market Requirement Document (MRD), we extract the design specifications needed to design a product for the next one or two years. This is how the research work progresses. Essentially, the MRD outlines the requirements and sets the market expectations, which is why it's called the "Market Requirement Document." From this document, we derive the design specifications for the product to be developed. Once we obtain these design specifications, we can determine the inputs and outputs required for that specific product. We've already covered this in the logic design concepts, right? This was the first step in the design methodology for creating a combinational circuit. Please try to connect these concepts here.
So, the design specification is extracted from the market research document. The market research document involves a group of members who work on identifying the requirements needed in the market. They extract all these requirements and use them to frame a product. Once they have framed the product, they obtain the design specification. In this design specification, they determine what inputs are required, what outputs are needed, and the functional specifications of the particular product. This is the design specification part. Once the design specification is known—meaning what the inputs are, what the outputs are, and what the product should do at the top level—all of this is considered from a top-level perspective. At this stage, there's no need to delve into the block-level details. So, in the design specification, we essentially extract information from a top-level perspective. From this design specification, we proceed to develop the product.
Architectural definition refers to the overall structure of a particular product, outlining the key components or sub-blocks required for its design based on the specified inputs and outputs. This definition is established at the top level, where the necessary blocks for the product's functionality are identified and framed. Once the top-level architectural definition is established, it is further divided into smaller, more detailed blocks, which we refer to as the microarchitecture definition. Microarchitecture involves breaking down the main top-level design into block-level designs, allowing for a more granular understanding of each component. For each microarchitecture, a detailed description or specification is written, ensuring that all aspects of the design are thoroughly defined and accounted for.
In RTL design, you'll be writing an RTL design. From here onward, your RTL design will be applied. Until this point—basically from here to here—our focus will be on the block-level portion. You'll define a top module, specify the set of inputs and outputs, and then decide what should go inside it. Once you develop the main architecture, it will be divided into smaller sub-blocks, which we call the microarchitecture. For each of these microarchitecture blocks, an RTL design will be framed, meaning an RTL code will be written for each of these sub-blocks. An RTL design is essentially about writing RTL code for each of these specifications. Is this clear? Any questions about this part? If you have any, feel free to ask.
Here's the corrected version of your text:

---

Any queries? You can unmute yourself and ask them. So, let's start with the RTL design or RTL coding part, which we have already studied. Once you complete the coding or finish the RTL design, that will cover the RTL design part of your entire design flow. Now, we move on to RTL verification. We have already discussed RTL verification. RTL verification is essentially about checking the functionality of your specific design. At this stage, you verify the functionality of your design by providing a set of inputs, simulating the design, and observing the outputs. This is the only point in the entire chip design process where you check the functionality by giving inputs and verifying the outputs. In other words, this is the simulation phase of your entire chip design. You won't perform this type of functional verification elsewhere, not even during synthesis or physical design.

--- 

This version maintains the original meaning while improving grammar and clarity.
Designing an entire chip involves verifying the functionality of the RTL (Register-Transfer Level) code, but you don’t simulate the entire chip at this stage. Instead, you focus on verifying the RTL code itself. During the RTL verification phase, you check whether the RTL code functions correctly according to the specifications. To perform RTL verification, you need to create a test bench. A test bench is essentially a simulation environment that provides input stimuli to the RTL code and verifies its output behavior against expected results.

Let’s break this down:

1. **RTL Verification**: You verify the RTL code you’ve written to ensure it behaves as intended.
2. **Test Bench**: The test bench is the tool used to simulate the RTL code. It generates input signals, applies them to the RTL design, and checks if the outputs match the expected results.
3. **Process**: When you write RTL code, you also write a corresponding test bench. The test bench acts as a virtual environment to exercise the RTL design under various conditions and scenarios.

So, the question is: How do you perform RTL verification? The answer is by writing a test bench. The test bench simulates the RTL code by providing input stimuli and validating the outputs. This process ensures that the RTL design functions correctly before moving to the next stages of the design flow.
In RTL verification, you determine whether the RTL design meets the specific requirements or fulfills the desired functionality. This is achieved by providing inputs to the RTL code and checking the corresponding outputs. Essentially, you are verifying the functionality of your entire design during this stage. Once the RTL verification is complete, you analyze the waveforms or outputs to see if they align with your design goals. If the design meets your goals, you proceed to the next stage, which is synthesis. However, if the design does not meet your goals, you must return to the RTL design stage to make the necessary corrections. Is this clear? So, this covers the basics of RTL design and RTL verification. Now, let’s move on to synthesis and the subsequent steps.
So, in synthesis, what exactly are we doing? When we talk about synthesis, what does "synthesis" mean? Can you provide some inputs? What do you mean by synthesis? Converting code into hardware? Yes, converting code into hardware. Any other answers? Generating a gate-level netlist? Generating a gate-level netlist? Okay, any other answers? Not seeing for timing delays—timing delays are considered in synthesis, but they come under STA (Static Timing Analysis). Now, in synthesis, you do consider timing delays, but the primary focus is on converting your RTL code into a gate-level netlist. One of you mentioned "converting your code into hardware," which is essentially correct. So, basically...
If you mean "converting into a gate-level netlist," then yes, that is true for hardware. However, if you mean "hardware" as a chip, then it is definitely incorrect to say that you get the exact chip out of it. There are many stages you must go through to complete the final product. For example, you need to go through the physical design phase to manufacture the entire chip. But when I say "delays cannot be synthesized," that is correct. Synthesis does not account for delays; it focuses solely on the RTL (Register-Transfer Level) code. The test bench is not synthesized—it is used for RTL verification. Only the RTL code itself is synthesized. Did you understand?
Yes, ma'am. Okay, so this is why everyone shouldn't get confused about RTL verification. The test bench that you write is just for checking the functionality. Once you know the functionality is correct, you won't move forward with that particular test bench. You won't be using that test bench anywhere; you'll drop that test bench and move on with the RTL code itself. You carry forward the RTL code itself for the next stages of your ASIC flow, ma'am. By that point, you only need to account for the delays. Those delays will be given as constraints to the synthesis tool. Okay? So here...
In logic synthesis, the gate-level netlist represents the design at the level of logic gates, such as inverters, buffers, multiplexers (muxes), encoders, and flip-flops. It is not at the switch level or MOSFET level. The conversion to the switch level or MOSFET level occurs during the physical design phase, specifically when you enter the PD (Physical Design) flow. We’ll cover that later, and once we discuss the PD flow, it will become clearer for you.
Okay, so here: In synthesis, basically what we are doing is converting the RTL code you write into an equivalent gate-level representation. For example, if you write something like "at the rising edge of the clock, output `a` should be transferred to `b`, or perform some arithmetic or logical operations," all those operations get converted into their corresponding hardware gates. If you have an adder, for instance, the synthesis tool generates the hardware implementation for that adder. When you complete the synthesis process, you get a gate-level netlist, which is essentially a `.v` file. This file is a software representation, not actual hardware, but it consists of interconnections between the gates. During synthesis, you are performing logic synthesis along with DFT (Design for Testability) insertion. DFT stands for Design for Testability, which means incorporating features into your design to make it easier to test. So, in order to test your...
You'll be adding scan chain information during the design process. This is done to test whether each and every block within your gate-level implementation is functioning correctly. Scan chains are added specifically to verify the operation of your gate-level netlist. DFT (Design for Test) insertion occurs during the synthesis stage. Prior to this, DFT is not inserted because, as mentioned earlier, functional verification is performed only at the RTL verification stage. After completing RTL verification, we simulate the design to check its functionality. Practically speaking, when the core is converted into a gate-level representation, it typically consists of millions of gates (or even more transistors). When simulating such a large number of gates, the runtime becomes significantly long.
It might take nearly months to get a particular output, so that is not a feasible solution. After converting it into a gate-level design, you cannot apply input stimuli and check its functionality directly. If you want to verify whether each circuit in your gate-level design works correctly or ensure its stability, you need to insert a scan chain or a DFT (Design for Test) module into your RTL code itself. This attachment of the DFT module occurs during the synthesis stage. In the synthesis part, we perform **logic synthesis**, which includes **DFT logic synthesis** and **DFT insertion**.  

Now, what do you mean by "logic synthesis"? Logic synthesis involves three main steps:  
1. Translation  
2. Mapping  
3. Optimization  

You must have studied these steps. So, when you...
Synthesis, or logic synthesis, involves three key steps: translation, mapping, and optimization. These steps are part of the logic synthesis process. Let’s break them down:

1. **Translation**: What does "translation" mean? Translation refers to the initial step where the RTL (Register-Transfer Level) code, which is the input to the synthesis tool, is converted into a technology-independent netlist. This netlist represents the design in terms of gates and other basic elements.

2. **Mapping**: After translation, the next step is mapping. Mapping involves converting the technology-independent netlist into a technology-specific netlist. This means selecting specific gates and cells from a library that matches the target technology (e.g., ASIC or FPGA).

3. **Optimization**: The final step is optimization. During optimization, the synthesis tool analyzes and refines the netlist to improve performance, reduce area, or meet other design constraints. This may involve restructuring logic, balancing critical paths, or minimizing power consumption.

All three steps—translation, mapping, and optimization—are performed automatically by the synthesis tool. That is why the process is considered an automated one. The input to the synthesis tool is the RTL code, which serves as the starting point for the entire logic synthesis flow.
To perform this translation, mapping, and optimization, you need to understand the concepts of logic design (LD). All the concepts of LD are implemented as algorithms to carry out these tasks. This is where the concepts of LD are applied. Now, what do you mean by "translation"? You’ve written an RTL (Register-Transfer Level) code, say for a D flip-flop. At the rising edge of the clock, whatever value is present at the D input should be sent to the Q output. During this process, the specific RTL code you’ve written should infer a D flip-flop. So, in the first stage, your code is translated into a block. At this stage, you identify that it’s a flip-flop—a block with one input, one output, and a clock pin. Your RTL code gets translated into this block representation, but you don’t yet know the detailed implementation.
What type of flip-flop is being referred to as "translation"? Now, what does "mapping" mean? The concept of mapping applies during the synthesis stage. At the synthesis stage, you will apply the target libraries. During synthesis, you will specify the library and technology you are using. For example, you might be synthesizing for a 40-nanometer technology and designing the product for a 40-nanometer process. You will target it for a specific foundry, such as TSMC. If you are targeting it for a 40-nanometer technology and a TSMC foundry—or any other combination, like a 40-nanometer technology and an Intel foundry—you need to include the appropriate details.
The library files for the specific technology and the specific foundry are included during the synthesis stage. Until this point, in the RTL design stage, you don't need to worry about which technology you're working with or which foundry you're targeting. That's why we say that an RTL code or an RTL design is technology-independent. The technology dependency of your product begins at this stage—specifically during synthesis—and continues through the physical design phase. From synthesis onward, the design becomes technology-dependent, meaning it is targeted for a specific technology and a specific foundry.

Now, what does "mapping" mean? During this stage, you apply the library files for a particular technology and a specific foundry. For each technology and foundry, you will...
The library files you'll receive will contain D flip-flops from the TSMC foundry for the 40-nanometer process, with different drive strengths. At the mapping stage, this flip-flop gets converted into `DF1`, where `DF1` is the instance name of the D flip-flop from the TSMC foundry targeted for the 40-nanometer technology. You must include the library files for the TSMC foundry's 40-nanometer technology before performing synthesis. The translation process itself is independent of the technology and the foundry; you're not targeting a specific library during translation. However, at the mapping stage, the tool attempts to map the translated blocks to the pre-built library files or library modules available. What are library modules? Library modules may consist of components like AND gates, OR gates,...
It might consist of an encoder, it might consist of flip-flops, and sometimes it might even include counters. So, how are all these individual blocks—such as gates, multiplexers, encoders, flip-flops, and counters—designed? These basic building blocks are designed at the transistor level, correct? Which domain of VLSI is responsible for designing these fundamental components? Full-custom engineers are the ones working on this. Full-custom engineers design each of these basic blocks and gates at the transistor level. They create the layout for these components, and a portion of that layout information is extracted. We call this extracted information a "frame view," and it will be available to us at the synthesis stage.
The instance of all this—if you name it `df1`, that will be an instance of the D flip-flop that exists in this library. When I say "flip-flop," it could be a D flip-flop, a T flip-flop, a JK flip-flop, or any other type. One instance of that will be named `df1` because you might have multiple instances of the same flip-flop in your design. Clear with this?

Now, what do you mean by "mapping"? All of you, please let me know if you have any issues with this concept. Bring them up. Yes or no—if this concept is clear? Translation and mapping: Okay, fine.

So now, the last step is **optimization**. In optimization, what we are doing is trying to reduce the number of blocks. The algorithm running behind the optimization process will apply Boolean algebra laws, K-map simplifications, and Boolean arithmetic laws.
Be applied here in order to reduce the number of blocks. If you can reduce the number of blocks, that particular optimization will take place at this third level of your synthesis stage. Clear with this? So, this is where your logic design (LD) concepts get applied in the tool. In the optimization portion of your synthesis stage, it is essentially an algorithm that is running. What concepts are implemented in those algorithms? You will have Boolean laws. In the first two or three days of your classes, you saw the simplification of equations. You worked on a few assignment questions to simplify a given equation so that you could reduce the count of gates. The main target is to reduce the gate count, which helps reduce the area, lower the power consumption, and improve the speed. So, in the optimization algorithm, those concepts are applied.
Loss minimization and K-map equations will be adopted for the simplification stage. That is about your synthesis stage in your design flow—that is, the third step. Is it clear for everyone? Any queries with this? So, the output you get from the synthesis stage is a netlist. Here, what you receive is not just code; you start with an RTL (Register-Transfer Level) code, and the input to the synthesis tool is also an RTL code. However, the output of the synthesis tool is a netlist. Again, it is a text file, but it will contain instances of your flip-flops and their connectivity. Is that clear? Let’s look into an example of this particular netlist file.
Sure! Please provide the text you'd like me to correct.
Okay, so this is an example of a gate-level netlist. You might have already seen something like this in the C-to-C class as well. This is a sample of the gate-level netlist that gets generated after the synthesis stage. You provide an RTL code as input, and this is the gate-level netlist that results from it. Here, the name of the module is shown, along with its top-level structure, which includes all the input and output ports. The input and output ports are defined accordingly. Now, here are the instances, such as the `counter` instance, which represents...
This is the name of your submodule. Here, "instance_counter" is the instance of your main module, which is called "counter." This particular "counter" block already exists in the library, and "instance_counter" is the instance of that block. These are the inputs to the counter block. Got it? So, this is how your gate-level netlist gets generated. These are the interconnections. Over here, you are using a D flip-flop, and for the D flip-flop, you have an input pin labeled "D," which is connected to net 60. This is your serial input, which is part of a test pin because you are including a scan chain in your design. We will cover the scan chain in more detail later. This is the scan enable pin, and this is your clock pin.
The Q pin of your D flip-flop is referred to as `Q`. Similarly, you'll have XOR gates, XNOR gates, AND gates, NAND gates, and inverters. These are all part of the library files. For example, `INV_X1` is the name of the library file targeted for a specific technology, such as the 40-nanometer TSMC foundry, as we discussed. Inside that 40-nanometer TSMC library, `INV_X1` is the module name. `INV_X1_U17` is the instance name given in the netlist for a particular project. So, when you perform synthesis, the synthesis tool generates this specific instance name. Is that clear? I'll continue with the presentation. This is about the netlist. A netlist is generated in text format, but it provides us with...
It provides information about the interconnections. Once you generate this netlist, you perform an LEC (Logical Equivalence Check). LEC is essentially a comparison to ensure that the functionality of the netlist matches the original RTL code you designed. As I mentioned earlier, functional simulation ends at the RTL verification stage. You won’t perform functional simulation moving forward. If you want to verify whether the functionality of the netlist is correct or whether the interconnections generated are consistent with your original RTL code, you use the LEC check. We refer to this as a Logical Equivalence Check. To ensure that the netlist has the correct functionality, you don’t provide input stimuli and observe the outputs. As I explained, the netlist consists of millions of gates, including inverters, multiplexers, flip-flops, counters, and more. If you were to generate stimuli and observe the outputs, it would be impractical due to the complexity and scale of the design.
It would take a really long time—it might even take months just to get the first simulation run. That is not practically feasible, which is why we rely on the LEC (Logical Equivalence Check). So, what happens in the LEC check? In this process, the netlist generated by the synthesis tool is compared with the original RTL code. Here, the connection is clear: the inputs for the LEC are the RTL code and the netlist. The netlist and the RTL code are compared using a specialized tool designed for logical equivalence checking. This tool performs the comparison to determine whether the blocks generated match the RTL design or if there are any mismatches. If any mismatches are found, the tool throws an error. In such cases, you may need to revisit the synthesis process and regenerate the netlist, or you may need to go back to your RTL design and modify the code. Remember, in your RTL design, every piece of RTL code you write...
Is not synthesizable, right? There are certain rules that you need to follow in order to write RTL code so that it is synthesizable. If you have written RTL code that is not synthesizable, in that case, you have to go back and change your code itself. Otherwise, you may need to go back to the synthesis stage and adjust the constraints that you applied. There are a few constraints that you might apply during synthesis, so you would change those and then regenerate the netlist for further comparison. 

LEC (Logic Equivalence Checking) is used to check for functionality, but here, we don't check functionality by providing input stimuli and generating outputs because that approach is not practically feasible. With a large number of inputs and outputs, simulation would take an extremely long time, making it impractical. Instead, we use LEC to verify equivalence. I hope this explanation of LEC is clear for everyone. Please ping with "yes" or "no" to let me know if you understand what LEC means.
Even in the API flow and the PD flow, we will discuss this concept of "LEC check." Have you understood about the LEC check? Please respond with "yes" or "no." Okay, great—we’ll continue. Next, we have the pre-layout STA (Static Timing Analysis). Here, the timing analysis is performed for the first time. Even if the functionality is correct, if the timing fails, the chip might still fail. For example, if you are designing a chip for automated products, such as an ATM machine, even though the functionality is working properly, if the timing requirements are not met, it becomes useless. Suppose you provide your inputs, but if you don’t receive the output (like cash) within one minute or two minutes, and instead, it takes ten or fifteen minutes—does that functionality serve any purpose?
Even though the functionality is correct—that is, it does dispatch the amount—but if it takes a longer time for the amount to be dispatched, timing is also very crucial in designing any product. Functionality is checked in this step, while timing is checked in STA (Static Timing Analysis). Since we are not generating the layout here, we only have a gate-level netlist with the interconnects of all the blocks, and we don't have layout information. We call this **STI** (Static Timing Information) check a **pre-layout STI**. In this pre-layout STI, we perform certain timing checks to ensure that setup constraints are being met properly, identify any violations in the setup, and check for any violations in the hold part of the timing. All of this is verified in the STA. Essentially, in this STA check, you are analyzing the timing information.
So, your particular product is targeted for a specific frequency—say, 3.2 GHz. If your product does not meet this target frequency of 3.2 GHz, it is considered a failure with respect to timing. In such cases, you need to apply certain fixes to address the timing issues. During pre-layout STA (Static Timing Analysis), if all the timing goals are met, you can proceed to the next part of your flow, which is the physical design. However, if the timing goals are not met, you must go back to the synthesis stage and make the necessary modifications.

Now, coming back to the physical design stage, ma'am: Is STA (Static Timing Analysis) done manually, or is it automated? Everything is automated; there is nothing manual involved here. All of this is part of an automated process. Of course, while most violations in STA are automatically fixed by the tool, a few violations may still require attention at the end.
Left out with ten or fifteen violations which cannot be fixed by the tool. In that case, you have to manually intervene and find out why those particular violations are occurring. Can you fix them manually by moving the cells? That is also done, but most of the violations will be taken care of by the tool itself. You should know that this process is done at both the block level and the chip level. First, you close your timing at the block level, but when you integrate it into the chip level, you’ll likely encounter more violations. So, you’ll have to close your timing again at the chip level. There is a separate tool for STA (Static Timing Analysis), a separate tool for LEC (Logic Equivalence Checking), a separate tool for synthesis, a separate tool for RTL verification, and a separate tool for RTL design. So, all of these tools play a role in the overall process.
The tools you use at each stage are different. For LEC (Logic Equivalence Check) verification, you use a specific tool, and for STA (Static Timing Analysis), you use another tool. At our center, we use the PT Shell tool provided by Synopsys, which is known as Primetime or simply "PT Shell." If the timing requirements are met, we proceed to the physical design (PD) stage. In the physical design, the first step is APR (Automatic Place and Route). During the API (After Place-and-Routing) stage, you define the boundaries and specify the core area, which is often referred to as the "core area." Then, the tool automatically places all the cells from the netlist. You have all the instances of different cells, right?
All the cells will be placed, and at this stage, you will obtain the layout information. Until now, you did not have the layout information—you only had the gate-level netlist, which was just a collection of instance names. In this API flow, you will generate the layout information. Each block will have its own layout information, and that layout will be placed within the specified area. The entire process is handled automatically by the API tool, which performs automatic placement and routing. While the tool will handle the placement, you still need to provide constraints and timing information to guide the tool on how and where to place the cells. Based on these inputs, the tool will perform the placement according to its algorithms and attempt to achieve the best possible arrangement. However, you may need to make some minor adjustments or fine-tune the placement of certain cells to ensure that the timing and other design goals are met.
This is where the layout information for each and every part of your cell is defined. Here, I’ve just drawn nine cells for simplicity, but in practice, your gate-level netlist will contain millions of cells. It could have anywhere from ten thousand to thirty thousand, fifty thousand, or even one lakh (100,000) cells, or even twenty lakh (2 million) cells. Each of these cells will typically have around ten to fifteen transistors. When I say "cell," I mean a single cell, such as an AND gate. For example, if you consider CMOS logic, an AND gate will usually have around eight transistors. If you go to the transistor level, the count will increase further. I’m giving you the count at the gate level, not at the transistor level. At the transistor level, the count will still increase significantly. Is that clear? So, this is your API (After Place-and-Routing) stage. Once your tool...
After completing the placement step, the interconnections in the gate-level netlist may vary. However, the tool must maintain the same interconnections as those specified in the gate-level netlist. If any issues arise, some interconnections might be missed. To identify and address these missed interconnections, you perform a **LEC (Logical Equivalence Check)**. At this stage, what exactly are you checking? Essentially, two things will be compared.  

In the LEC check, the comparison is typically done between a netlist and an RTL code. However, in this case, the LEC check involves comparing **two netlists**: the original gate-level netlist and a placed netlist. The input for the LEC check is the original netlist and the placed netlist. The arrow indicates that one input is the original netlist, and the other input is the placed netlist. This placed netlist will be compared with the original netlist because the original netlist has already been verified against the RTL code, ensuring that it functions correctly. Since the original netlist is already confirmed to be functionally accurate, the placed netlist must match it to ensure no interconnections are missing or altered during the placement process.
At least, it gets compared with the placed netlist because, when the tool places the cells, it performs optimizations to meet your timing, area, power, and speed requirements. It might add buffers, remove buffers, upsize cells, downsize cells, or make other changes. As a result, the netlist gets modified. However, the modified netlist should remain logically equivalent to the original netlist. To verify whether it is logically equivalent or not, you perform a LECC (Logical Equivalence Checking) check. In this LECC check, you compare the modified netlist with the RTL code. Don’t get confused here—it’s important to understand that this is the original netlist, while the netlist you receive is a modified version. Why? Because it is the output of an API tool, which performs optimizations to meet your timing, area, power, and speed goals. Is this clear? Ma’am, this netlist is not...
It seems like there is some confusion in the phrasing of your statement. Here’s a corrected version:

---

The RTL netlist is not checked until after logic synthesis. Before logic synthesis, you only have the RTL code. After the RTL code undergoes synthesis, you get the netlist. At this point, you need to check the netlist. Throughout the entire flow, you are working with the netlist, whether it’s for LEC (Logic Equivalence Checking) or any other step. The netlist is the output of synthesis, and it is the basis for all subsequent checks and verifications.

---

Let me know if you need further clarification!
Here is the corrected version of your statement:

---

Yes, ma'am. After the initial check, you will get the original netlist. Yes, ma'am, this is the first netlist that we have. It is not a checklist; we simply provide the input for the next step, which is LEC (Logic Equivalence Checking). Here, this netlist goes through LEC. Once it passes through LEC, you receive the output, which is the final netlist. This final netlist is then given as input for the next stage. When you see this stage, it means that the netlist has successfully passed through all the previous design steps. If it hasn’t passed, the loop repeats until it is verified. Yes, ma’am, understood. So, after completing the LEC check, you move on to the next part of the flow, which involves parasitics.  

--- 

Let me know if you need further refinements!
So, what do you mean by "parasitic extraction"? If you don’t know, it’s fine—no issues. Parasitic extraction refers to the process of identifying and quantifying the unintended electrical effects caused by the physical layout of a circuit. Specifically, it focuses on the interconnections between transistors and other components. When we talk about RC extraction, we’re referring to the resistance (R) and capacitance (C) associated with these interconnections. 

The RC values represent the parasitic effects of the wires connecting the transistors and other components. At the output of a transistor, for example, you’ll encounter these parasitic effects. However, when we say "parasitics," we’re being very specific: we’re referring to the parasitic effects of the interconnecting wires themselves. Each standard cell in the design is connected through these wires, and the interconnections introduce additional resistance and capacitance that can impact the overall behavior of the circuit.
These interconnecting wires will be present, and you’ll be using metals for these connections. When I mention "metal," it has a width and a length. Any semiconductor material with dimensions *l* (length) and *w* (width) will have resistance because the equation for resistance is \( R = \rho \frac{l}{w} \), where \( \rho \) is the resistivity. So, it will have resistance.

In higher technology nodes, such as 250 nm, 180 nm, and others, you could often neglect the resistance and capacitance of these interconnecting wires. However, as technology has shrunk—now we are at 7-nm, 5-nm, and 10-nm technology nodes—we can no longer ignore the *R* (resistance) and *C* (capacitance) values of these interconnecting wires. These wires effectively behave as equivalent resistors and capacitors. You can represent them as *R* and *C*, but these values are typically not included...
Till this stage, after the API (Analog Physical Integration) flow, we will consider this parasitic information. Then, you will perform an STA (Static Timing Analysis) check in the pre-layout STA. In the pre-layout STA that you do here, we are not concerned about the interconnecting wire resistance (R) and capacitance (C) because these R and C values cause the signal to be delayed. The resistor and capacitor act like a charging and discharging loop. You know the behavior of R and C, right? The typical R and C combination forms an RC network. Whenever you have an R and C, the signal gets delayed. Signal delay affects the timing. In the pre-layout STA that we perform after the synthesis stage, we do not include parasitic information because the core area is not defined yet. We don’t know exactly where the cells will be placed, nor do we know the length of these wires. We only know the block-level information at this point.
The only information we have at this stage is the connecting wires to this block. However, in the API (After Place-and-Routing) stage, you have a defined core area, and you know exactly where the cells are placed and how long the wire lengths are. That’s why you perform parasitic extraction and then conduct a post-layout STA (Static Timing Analysis). When you perform post-layout STA, you account for the effects of resistance (R) and capacitance (C) and incorporate them into the timing analysis. This is the key difference between pre-layout STA and post-layout STA. Do you understand?  

So, again—if the timing is met, you proceed to the next stage. If the timing is not met, you go back to the API flow and adjust the place-and-route settings until the timing is satisfied. Once the timing is met, you perform physical verification. During post-layout, some modifications may be made, so you need to re-verify the physical integrity of your netlist. At this point, you now have the layout information.
As well, once you complete this step, you will proceed to the DFM (Design for Manufacturability) and OPC (Optical Proximity Correction) checks. DFM stands for Design for Manufacturability. Issues related to DFM often do not manifest immediately after the hardware is manufactured. Instead, these DFM issues tend to appear after prolonged use of the product. For example, if you have designed an IC for a mobile application, due to a DFM issue, you might experience problems such as your mobile device hanging or taking a longer time to load certain applications after one or two years of use. These are examples of DFM-related issues. However, we aim to address these DFM issues during the manufacturing stage itself. These issues are typically studied based on feedback from previous products that were released. Complaints about similar issues often surface after two...
The issue arises after three years of use. When they brought the product back and conducted a study, they found that the problem was related to DFM (Design for Manufacturing) issues. There should be a particular spacing followed when making interconnections, and the width of the interconnecting wires should meet a minimum requirement to resolve these DFM issues. Essentially, the DFM issue occurs because charges accumulate at a particular point over time. To clarify: if there is a block here and another block here, with interconnecting wires between them, and one more wire nearby, as the device is used for two or three years, charges accumulate at one particular wire. Over time, these charges build up, causing the two wires to short. When these two wires short, the...
The entire chip will start behaving differently because its functionality will be altered. Due to this, you may notice that your product begins to exhibit unusual behavior. For example, in a mobile application, the entire mobile phone might freeze, or it might take longer for apps to display. Sometimes, the device might suddenly turn off due to power issues. All these problems have been analyzed, and during the design phase, designers take precautions to address them. While they cannot completely eliminate these issues, they do account for Design for Manufacturability (DFM) concerns to minimize them. Got it?

After that, the final step is tape-out. The abbreviation "DFM" stands for "Design for Manufacturability." Once you complete the final routing in the APR (Automatic Place and Route) flow...
You have the cells placed here, and for all these cells, you perform routing. When I say "routing," it is a highly complex process. The wires will be placed very closely together, with two wires running extremely close to each other, separated by the minimum allowed spacing. Additionally, one more vertical or horizontal wire may run over them, creating a dense network of routes in your design. With so many routes in your design, the question arises: Can this design actually be manufactured? This is what you check during the DFM (Design for Manufacturability) stage. 

DFM, or Design for Manufacturability, involves verifying whether, after prolonged use of the product, there will be any issues related to the accumulation of charges. If the routes are placed too close to each other or if two cells are positioned very near one another, there could be a risk of charge accumulation, which might alter the functionality of the design. This is the primary concern addressed during the DFM check.
That is what you'll be checking here. OPC (Optical Proximity Correction) is nothing but your optical proximity check. In this OPC, due to radiation issues, whether there are shorts or opens occurring is checked. These checks are performed during manufacturing itself. Although these issues may occur after prolonged use, they found the repercussions of all the products that were released. When they analyzed those products, they discovered that there are issues related to placing two cells very close to each other. Even though it doesn't cause any DRC (Design Rule Check) issues during the design stage, prolonged use leads to problems with charge accumulation. All these factors will be checked during the design phase itself. Only after everything is ready will you proceed to tape-out. Is this clear for all of you?
In DFM (Design for Manufacturing), how will they check for issues? The design is already complete, so how do they verify DFM-related concerns? If there are any issues, you have to fix them manually, especially regarding connections. All connections must be finalized before you can check for DFM compliance. You cannot perform DFM checks before completing the connections because, in DFM, the design is still in the digital or virtual stage—it hasn't been physically manufactured yet. Physical manufacturing occurs during the tape-out stage, where the design is sent for actual production. DFM checks are conducted while the design is still in the layer stage, before physical manufacturing takes place.
Itself, it is still on the software only. So, after you complete all the routing, you run a DFM (Design for Manufacturing) check. If there are issues in the DFM check, there are certain fixes that need to be adopted. You have to apply those fixes. These fixes involve different methods that are used to resolve the DFM issues. You must address the DFM issues using those fixes or methods. Only after that can you send it for tape-out. Once you send it for tape-out, you finally get the physical chip in your hand—a hardware chip with pins. Before that, you don’t have a physical chip; it is just a software representation. Is that clear, ma’am? Is there any need to send the output of the DFM to the post-layout stage? No, after the DFM check, you don’t need to do that.
Ma’am, after APR (Advanced Physical Design) and the first physical design phase, we generate the layout, right? Yes. After this APR process, we have the first layout. For that layout, we’ll perform LECC (Logical Equivalence Checking), correct? So, LECC is done for the layout, right? Okay. Next, we move to parasitic extraction, where we extract the RNC (Resistance, Capacitance, Inductance) values of the layout. Specifically, we extract the RNC values of the interconnecting wires within the layout. 

But why do we extract all these values? What is the purpose of this? Well, from the parasitics, we obtain the RNC values, which affect the signal behavior. Due to resistance (R), capacitance (C), and inductance (L), the signal experiences delays. For example, the capacitor needs to charge and discharge, causing a small drop in the signal across the resistor. This results in increased time delays, right? Ma’am, so this has to be...
In STA (Static Timing Analysis), okay. So, in physical verification, what happens? In physical verification, you'll be checking the functionality of the layout. Yes, okay. What is OPC (Optical Proximity Correction)? Thank you, ma'am. Any queries? Ma'am, again, how will the charges accumulate? Ma'am, again, how did you say the charges will accumulate? By accumulation only, but over a prolonged use, there might be several reasons, such as ESD (Electrostatic Discharge) or wire issues. When the pins of the IC come into contact,...
With a charged particle—whether it’s a human hand or any other charged particle—from the pins of your IC, the charge can enter your integrated circuit and get deposited at a particular point. To avoid ASI (Analog Signal Integrity) violations at the input pins and across all input and output pins of your circuit, you’ll have an Electrostatic Discharge (ESD) unit. This ESD unit will redirect all the charged particles to the ground level. However, due to some issues, especially over prolonged use of your product, these charges tend to accumulate. This accumulation primarily occurs because of the ESD mechanism itself, as well as variations in voltage fluctuations. These factors cause the charges to build up due to electrostatic effects.
The charges get accumulated due to variations in the current, and there are many other reasons for this. Practically, this phenomenon occurs in almost every product, which is why manufacturers provide warranty and guarantee periods for their products. If you look at the warranty and guarantee terms, they typically won’t exceed ten years because, by then, there will be a significant buildup of charges, and the product may no longer function properly. At that point, you would have to replace or dispose of the product. Got the point? Yeah, yeah, man. Okay. Before we continue, let’s take a break. It’s 11:40 now. Let’s take a break for ten minutes and resume the session at 11:50. Is that okay with everyone? Yes, ma’am. Okay. Please be back at 11:50. We’ll resume the session then. So, we’ll continue with...
So, we have seen the overall flow before we took a break. Now, we’ll continue with this. What do you mean by “flat synthesis” and the API flow? When you say “flat synthesis,” it means your entire architecture is not divided into different blocks—it’s a flat design. Your RTL design will have only a functional hierarchy, without any sub-blocks. So, the entire RTL design will be synthesized in one stage, rather than being split up into different blocks. Once your synthesis is complete, you’ll move on to the API flow. In the API flow, you’ll perform floor planning, power planning, and the place-and-route stage. After that, you’ll check your STA (Static Timing Analysis) and then verify your DRCs (Design Rule Checks), LVS (Layout Versus Schematic), and DFM (Design for Manufacturing). Finally, you’ll proceed to tape-out. DRC refers to Design Rule Check, which ensures that the minimum...
The minimum spacing, what should be your oxide thickness, what should be the length of your interconnecting wires, and what should be the thickness of your diffusion—all of that is covered in DRC (Design Rule Check). LVS (Layout Versus Schematic) is a check to ensure that the layout matches the schematic. All of this is part of the flat synthesis flow in APR (Advanced Placement and Routing). In the previous case, we saw the block-level split-up, which is essentially the division of the entire architecture definition into microarchitecture, also known as a flat design. Here, we will be splitting the blocks into a flat design, meaning there is no block-level hierarchy—it is the entire functional hierarchy. Then, we proceed to the synthesis stage. This entire process is part of the API (Application Programming Interface) flow. Next, we perform STA (Static Timing Analysis), followed by DRC, LVS, DFM (Design for Manufacturing) checks, and other...
Now, if you consider the design blocks, we have discussed the architectural definition, and your main architecture gets split up into microarchitecture. This means it gets divided into different blocks. So, how do those different blocks look? This is about the design blocks. Here, your full chip design is shown, and in this case, my full chip has been split up into three different blocks: Block 1, Block 2, and Block 3. Each of these blocks contains internal logic. A set of engineers works on Block 1, a different set works on Block 2, and another set works on Block 3. So, three different sets of engineers work on each of these blocks. With respect to each individual block (Block 1, Block 2, and Block 3), the PD (Physical Design) engineers will close the timing, DRC (Design Rule Check) issues, LVS (Layout Versus Schematic) issues, and everything else. Once these issues are resolved, these blocks are brought into the top-level design. When you merge all these top-level designs at the top level...
You'll be doing all the checks again—you'll perform the LEC (Logic Equivalence Checking) check, STA (Static Timing Analysis), and everything else. If there are any issues, they definitely need to be fixed with respect to the top-level design. As you can see here, these are the timing paths between different blocks, the timing blocks within each block, and the flip-flops at the interfaces of your blocks. This is how your design blocks are merged into the top-level portion of your chip.  

If you consider a hierarchical synthesis approach (as opposed to a flat synthesis), in the previous stage, we saw a flat synthesis. In hierarchical synthesis, you start with an RTL design that represents the functional hierarchy. After synthesizing it, you'll perform design partitioning. During this step, the entire functional hierarchy gets split into different blocks, and we refer to this as the physical hierarchy. Now, you will apply the floor plan and power plan...
And block constraints for individual blocks. For Block 1, Block 2, and Block 3, you’ll be applying these constraints, including floor planning, power planning, and blocking constraints. Again, in each of these blocks, you’ll have a block-level RTL (Register-Transfer Level) code. You’ll perform synthesis, PNR (Place and Route), STA (Static Timing Analysis), and DFMs (Design for Manufacturing) checks. You’ll also have different timing models, both physical and timing models, so that you can merge them into the top-level design from these individual blocks. You assemble them to complete the entire top-level design. Once you assemble them, you follow the same process: the place-and-route stage, perform STA checks, conduct BFM (Boundary Functional Model) checks, and then proceed to the final tape-out. Most practical designs, especially chips that are manufactured, follow this hierarchical flow. In contrast, in flat synthesis, you won’t have individual sub-blocks; it’s just one block, and the entire flow is handled as a single entity.
In one stretch—that’s it. So, this is about hierarchical synthesis. Now, coming to the APR (Automatic Place and Route) stage, what do you mean by “automatic place and route”? Automatic place and route refers to the process where, in the APR stage, the layout of the ASIC (Application-Specific Integrated Circuit) is generated automatically by an APR tool using the gate-level netlist produced by the synthesis tool. The input to the APR tool is the gate-level netlist along with the constraints you apply. The APR tool will automatically place all the cells from the gate-level netlist within a predefined area space. A floor plan has already been defined, or a predefined area is provided to the tool. Within that predefined area, the APR tool will place all the cells. So, what are the different steps involved in realizing the layout or used in the APR flow? These are the different steps: **floor planning**, **power planning**, **placement of standard cells**, **clock tree synthesis**, and we also...
CTS (Clock Tree Synthesis) is followed by the final step, which is routing. Some of the tools commonly used for the APR (Automatic Place and Route) flow include Olympus from Intergraph, ICC (Innovus Implementation System) from Synopsys, and Silicon Ensemble from Cadence. In our PD (Physical Design) flow, we are using the ICC tool from Synopsys. These are the inputs and outputs of the APR tool. Here, my APR tool is as follows:  

The first input is the gate-level netlist. Next, I provide the timing constraints, which are stored in a file called the SDC (Synopsys Design Constraints) file. The SDC format was originally developed by Synopsys, which is why it is named "Synopsys Design Constraint." Since then, the format has become widely adopted, and everyone follows the same constraint specification. Therefore, we refer to it as an SDC file.
The **Synopsys Design Constraint (SDC)** file is widely used because it was originally designed by Synopsys and adopted by almost all vendors. We still use this SDC format for timing constraints. So, what kind of information is included in the timing constraint file? The timing constraint file specifies important details such as the frequency of your clock. If there are multiple clocks, it includes the frequencies of those different clocks. Additionally, it defines any delays that need to be applied to the circuit, such as input delays or output delays. All of this information is captured in the timing constraint file.

For example:
- What should be the maximum capacitance at every point in the circuit?
- What should be the maximum transition? (When we talk about "transition," we’re referring to the switching behavior of signals.)
- What should be the maximum load (maximum capacitance)?
- What should be the maximum fan-out?
- How much delay should be allowed?
- What should be the clock period?
- What should be the input delay?

All these parameters help define the timing and performance requirements for the design.
The output delay and all that information is specified in the timing constraint file. After that, you will need the following for the design: timing constraints, a gate-level netlist, and scan chain information. These are all related to a specific technology. For example, if you're designing for a 40-nanometer technology and targeting an Intel fab, you'll obtain a technology library specific to that fab. What information is contained in the technology library? The technology library includes details about all the gates, flip-flops, and other cells used in the design. It contains comprehensive information about these gates, such as their characteristics, delays, and other relevant parameters.
The technology library will contain various types of information. It includes details about all the basic gates, such as their functionality, timing information, and layout information. The timing information specifies how the gates behave in terms of delays and setup/hold times. The layout information is provided in formats like LEF (Library Exchange Format) and DEF (Design Exchange Format), which describe the physical characteristics of the gates. These formats allow for an extracted view of the layout, meaning they contain only the necessary layout information required for making interconnections, rather than the full layout details of the entire block. So, the layout information is presented in an extracted format, using one of these standardized formats.
Given that this is either a LEF (Library Exchange Format) file or a DEF (Design Exchange Format) file, all the blocks in the technology library won’t have the full layout information. Loading the entire layout information into your API tool would take a significant amount of time, and the run times would be too high. Since you don’t need the complete layout information, only the portion required for interconnections is extracted. If that information is sufficient, only the relevant views of the particular cell will be extracted. This is known as “left-handed depth view” or “interconnecting models.” That’s how R (resistance) and C (capacitance) values can be extracted. 

In network analysis, you have different topologies, such as star topology, tree topology, and others. These are different types of interconnection models. How can you simplify your interconnections in terms of R and C? Even when a transistor is in the on or off state, it behaves like a resistor. All of this needs to be simplified. So, what type of interconnect models are you using to simplify your...
The R and C values, along with models, are also included. Additionally, timing scenarios specify which corners you are targeting for your APR checks—whether you are performing them for the maximum (max) corner, minimum (min) corner, or typical (main) corner. We will discuss timing scenarios, corners, and max/min corners in more detail during tomorrow’s session. For now, just understand that timing scenarios define the conditions under which the design needs to be checked. Typically, setup checks are performed for the max corner, while hold checks are performed for the main corner. These corners must be specified in your inputs.

Now, what are the outputs of your APR tool? The primary output is a layout of the gate-level netlist. Additionally, you receive the gate-level netlist in a layout view format. You also get a set of timing reports, which provide information about errors and warnings. These reports indicate whether there are any violations in specific parts of the design. If there are violations, you need...
Fix them, and then a log file is generated. From the log files, you can search for errors or warnings that are generated at each step. Additionally, you’ll have QR (Quality of Results) reports. Analyzing these QR reports provides a better understanding of the design. If there are any violations related to maximum capacitance, maximum transition, or maximum fan-out, you must check these in the QR reports as well. Furthermore, you’ll also have information about the scan chain, such as whether the scan chain you’re attaching is successfully validated or not. This information is available at the output of your API tool.  

These are the inputs and outputs of the APR (Advanced Physical Design) tool. Next, we’ll look into the APR flow. We’ve already discussed the API flow here. In the API flow, you have the following steps: floor planning, power planning, placement of standard cells, clock tree synthesis, and routing. That is explained...
In detail, here are the different steps in the API (Advanced Physical Design) flow. When you work on a PD (Physical Design) project or learn about a PD project, this is the entire flow of your PD project:

1. First, you’ll start with **design and timing setup**.
2. Then, in the second stage, you’ll perform **floor planning**.
3. After that, you’ll do **power planning**.
4. The tool will handle the placement, but you need to provide appropriate input to ensure the tool performs the best placement.
5. Next, the tool will perform **clock tree synthesis (CTS)**, and you need to give input in a way that ensures the tool does the best CTS.
6. Finally, the tool will handle **routing**, but again, you need to provide input to ensure the tool performs the best routing.

These are the different stages of your API flow.

Now, what does “design and timing setup” mean? It involves loading the required libraries and the technology file. We will use a scripting language to load the required library and the technology file. In this case, we are using the ICC tool (ICC stands for **Innovus Custom Compiler**). The ICC tool of...
The Synopsys vendor provides the ICC tool, and in that particular tool, we use a Tcl script to load the libraries and the technology file. You have already learned about Tcl, right? Tcl scripting is not something new to you. Only shell scripting has been covered so far, but Tcl scripting will be introduced starting from Thursday onwards. Tcl scripting is one of the scripting languages used to load the technology files. While shell scripting can also be used, we are opting for Tcl scripting. Even Python scripting can be used to load the library and technology files, but for now, we are focusing on Tcl.

That is the first part: **design setup**, which involves setting up the design and timing by loading the required libraries and technology files from an appropriate path.

Next, we move to the **floor plan**. The floor plan is a process used to determine the placement of macro cells. What do you mean by macro cells? Macro cells are typically memory cells. Memory cells are nothing but IP (Intellectual Property) blocks that we obtain from...
The particular target library specifies that memory macro cells or memory cells should be placed manually. Input/output cells and macro cells will be placed manually by the designer, while the remaining standard cells will be placed automatically by the tool. The process of placing these macro cells, input/output cells, and defining a region for the placement of standard cells is referred to as **floorplanning**. For example, if I have a core area defined here, this is the core area that is defined. I...
This is the core area that is defined. You have your netlist, and from the netlist, these are the macros that are generated, along with the set of standard cells (the pink ones represent the standard cells). So, what does "floorplanning" mean? Macros are essentially memory blocks. The placement of these macros needs to be done manually. You have to decide where these macros should be placed—whether they should be placed here, there, or somewhere else. The process of deciding where to place these macros is called **floorplanning**. Once the macros are placed, the remaining standard cells (the pink ones, which are your standard cells) will be placed automatically by the tool. However, you still need to...
You need to allocate a continuous core area space for the standard cells to be placed. This is referred to as the floor planning stage. Now, what does "power planning" mean? Power planning involves adding metal wires to connect the power pins to all macros and standard cells. Both macros and standard cells have power pins, specifically VDD (positive supply voltage) and VSS (ground) pins. Here, the VDD and VSS pins need to be connected. To achieve this, you create power straps. 

This is how you create your power straps: you create one power strap in this direction, another power strap in this direction, and then add horizontal power straps as well. Essentially, you are not manually drawing these power straps; instead, you write a script to automate the process.
There are certain commands where, when you provide inputs such as length, width, and spacing, the tool will automatically create the necessary structures. This process uses a reference file, and you need to provide the inputs based on that reference file. The process of creating these power straps is called **power planning**. Power planning involves defining the power distribution network for the design.

Standard cells are instances of basic logic gates, such as AND gates, OR gates, NAND gates, or flip-flops. When I refer to **macros**, these are typically larger blocks, such as memory modules. IPs (Intellectual Properties) that are generated often include large sets of memory blocks. For example, if you are designing a processor, that processor might require a significant amount of memory. Some of this memory could be cache memory, while others could be register files. When dealing with such large memory blocks, they are generated using a reference file or a specific methodology to ensure proper integration and functionality.
Block, so that is nothing but your macros. The displacement of the macros can be done manually, although you can also use the tool to assist with it. However, placing them manually often yields a more optimized result. After that, the remaining standard cells will be numerous—around fifty thousand or even one hundred thousand—and you cannot place each and every standard cell inside the core area by hand. Of course, the macro count will be much lower, typically around fifty, sixty, or forty macros. Since there are fewer macros, you can place them manually, one by one. You can select them, move them wherever you want, and place them accordingly.

Once that is done, the standard cells need to be placed automatically. This is handled in the placement stage. But before that, you must create power straps, specifically the VDD and VSS connections. Each macro and each standard cell will have VDD and VSS pins, so those VDD and VSS pins must be connected correctly.
You need to create the power straps like this, but you don’t need to draw or manually create them. Instead, you simply provide the inputs, and the tool will generate the power straps based on those inputs. The quality of your inputs determines how well the tool creates the power plan. Of course, when you create a power plan, you must always meet the IR drop requirements. 

Now, what do you mean by **IR drop**? Imagine your VDD port is over here, and the VDD supply is taken from this port. As you move inward, since this is a metal structure, there is resistance throughout the metal. Due to this resistance, the voltage gets dropped as it travels. For example, if the voltage at the VDD port is 1.1 volts, by the time it reaches a point further inside the power grid, the voltage might drop to 1.0 volt, and even further, it might drop to 0.9 volts. The question is: How much of this voltage drop is allowed? Or, in other words, how much IR drop is acceptable?
That is specified in the IR drop goal. You have to set a goal or a target for the tool to check for the IR drop. Based on the target you set, the tool will generate a map and highlight areas where there is a voltage drop. It will show regions in different colors—red, blue, or green—to indicate the severity of the drop. For example, it might highlight a particular region of your entire core area, indicating a drop of 22%, 33%, or more. In such cases, you would need to go back and adjust your power planning, changing parameters like the width, spacing, or pitch of the power rails. This process is iterative and often requires multiple adjustments before stabilizing. That’s why the PD (Power Distribution) flow is very time-consuming; you won’t get the desired results in a single attempt. Instead, you have to iterate through several cycles to achieve the required outcomes.  

Now, moving on to placement: Placement is simply the process of automatically arranging standard cells within the design.
Rows are created during the floor plan stage so that the timing constraints and the constraints specified in your SDC (Static Timing Constraints) file are met. The tool will automatically place the standard cells inside the core area because we have already created a power strap during the power planning stage. With respect to that power strap, the tool will place all your standard cells. However, while placing them, the tool will check against the timing constraints you provided in the SDC file.

Once the placement is complete, inside your entire chip, all the sequential cells will be triggered by a clock. Therefore, the connection of your clock is critical. Before you proceed with signal routing, you must first build a clock routing network. This stage is called **Clock Tree Synthesis (CTS)**. In your entire chip, if this is my chip and there are multiple blocks (I’m drawing just one or two blocks here), along with one or two standard cells...
And say your clock port is here—this is your clock port. From this clock port, this is the clock pin of the first block; from here, this is the clock pin of the second block; and from here, this is another clock pin. So, how best you can route these clock nets is handled in the CTS (Clock Tree Synthesis) stage. The process of connecting the clock pins of all the sequential cells in your IC to the clock net in such a way that the clock skew is minimized is called **clock tree synthesis**. If the clock skew is minimized, the number of timing violations will be reduced. Clock nets are the most toggling nets and also the most power-hungry nets, which is why we build the clock network first. After that, the final step is **signal routing**. Signal routing refers to the remaining routing tasks, such as routing the data (D) pins, Q pins, and other inputs and outputs. This is essentially the signal routing phase, and it is performed after the clock routing is completed. So, signal routing is done at the end.
Clear with this? Any doubts? Okay. So, the same concept is defined here. What is **floor planning**? It is the process of determining the placement of macro cells. A good floor plan achieves high **core utilization** and reduces the chip area. Based on your floor plan—specifically, how well you place your macro cells—the core utilization is decided. What is **core utilization**? We will look into it shortly. Usually, macro cells are placed at the periphery of the chip so that the center area remains a continuous core region. If this is your chip area, the macro cells you have need to be placed at the boundaries. You try to place the macro cells or memory cells at the...
Boundaries are defined so that a continuous space is left out for the tool. This remaining space is reserved for placing all the standard cells. This is one of the guidelines we follow during the placement of macros. For a full-chip floor plan, both I/O cells and macro cells need to be placed. If you're working at the block level, you are only placing the macros. However, if you are working at the chip level, you'll be placing both macro cells and I/O cells. Additionally, the center cell rows for the placement of standard cells are determined at this stage, after the floor plan. Next, you will perform the power planning. The rows of the standard cells are fixed because each standard cell has a fixed height. When you say "standard cell," all standard cells will have the same fixed height. This is how the design follows consistent guidelines.
The width might vary, but the height is always fixed. The height of the standard cell is consistently the same. After placing your macros in the core area, you will create the VDD/VSS power lines for the standard cells. Since the height of the standard cells is fixed, you will place these VDD/VSS rails, and all the standard cells—whether they are wider, medium-width, or the smallest/thinnest ones—will align with these rails. This ensures that all standard cells fit properly within the designated area.

Now, if you consider a chip-level floor plan, the process is similar. First, you define the die area. Then, you place the I/O cells, followed by the macros. After that, you create the VDD and...
It says "power grids" (PG stands for power grids), and then you proceed with the automatic place-and-route stage by adding placement and routing blockages. This is done at the entire chip level. So, this is how a chip-level floor plan looks. If you look closely, these are the I/Os, and these are the macros. The macros are placed on the periphery, specifically on the outermost ring, while the center portion remains empty for standard cells to be placed later. On all sides, you can see additional I/Os. In a chip-level floor plan, I/Os and macros are placed first.  

If you consider a block-level floor plan, you define a block area, place the ports (instead of I/Os), and then place the macros. Next, you create your power grids...
Then, you add placement blockages and proceed with the APR (Automatic Place and Route) flow. This is how your block-level floor plan looks. If you observe here, in the block-level floor plan, you have ports (not input/output ports, just general ports), and these are the macros. The remaining area is reserved for standard cells. Do you see the difference between block-level and chip-level floor plans? In a chip-level floor plan, you will have I/Os (input/output ports) in this section. Inside that, you place your macros. Here, you only have ports, and these are the macros you are placing. This is your floor plan for a block-level design. For a chip-level design, the only difference is that you will have input/output ports.

Next, let’s move on to core utilization. What does “core utilization” mean? Core utilization refers to...
The percentage of the core area that is used by the standard cells and the macros is referred to as **core utilization**. If you consider this particular core area, say these are the macros, and these are the standard cells, the total core area is divided by the combined area occupied by the standard cells and the macro cells. When you add up the area occupied by the standard cells and the macro cells and divide it by the total core area, you get the core utilization.  

When you place these macros in an appropriate manner, the core utilization can be optimized to its best possible value. Ideally, we aim for 100% utilization at tape-out, but practically, the range is usually around 80% to 85%.
So, initially, at the first stage—soon after you place the macros and without any optimization—when you place your netlist, the starting netlist utilization should be around 60% to 75%. That is, at the floorplan stage, when you check the utilization, it should be around 60% to 75%. Then, after placement, after CTS (Clock Tree Synthesis), and after routing, the utilization should be around 80% to 85%. This is what is meant by **core utilization**. If the core utilization is low, say 75%, it means that out of 100% available area, only 75% is occupied. What does this imply? You can still reduce the core area. Instead of having such a large core area, you can shrink it to a smaller size and then perform the placement. This saves a significant amount of space. Your goal is to try to minimize the core utilization as much as possible within the given space. Any questions regarding this?
Okay, next, we’ll move on to the next topic, which is **core-limited die**. So, when I say the **core area**, this is my main core area. I’ll place a boundary around it so that I can place the input and output (I/O) ports over here. This is a **core-limited die**, meaning your I/O cells are placed in this portion. The I/O cells are placed here, and the center portion is the core area, where I’ll place the standard cells. When the I/O cells are placed, they are not placed continuously. See, this portion is where the I/O cells are placed, and the green area is an empty space. This is an empty space, so this is one way of placing your I/O cells. This is called a **core-limited die**. This type of spacing is referred to as a **core-limited die**. Next, you have a **pad-limited die**, so in this...
The IO (input/output) cells or patches determine the die size. This die size represents the smallest boundary, or the innermost boundary, that you have. This is your main core area, and this defines your die boundary. In other words, this is your die size. The input and output cells determine how thick the die size should be. As you can see here, there are no empty spaces; all the available space is occupied by the IO cells. This configuration is known as a pad-limited die. This is one way of arranging your blocks, and there are different methods for configuring your blocks.  

Now, moving on to what makes a good floorplan: As I mentioned earlier, a floorplan involves placing the macro cells inside your core area. Macro cells are typically placed at the periphery, creating a continuous core area for the standard cells to be placed. This arrangement...
This is my core area, and in this core area, I am placing my macros here. By doing so, I leave a continuous core area for the standard cells to be placed. This is one of the guidelines you need to follow when creating a good floorplan: minimizing the routing distance between the pins of the macro cells and the standard cells. When you are placing the macros—or when placing these macros—you should be very careful with respect to the pins and the ports. These are the ports that are being placed here, and these are the ports you need to consider. Try to place these macros near the ports they are connected to. For example, if I have placed this macro, but this macro has a connectivity here, this results in a long routing path. Instead of that, what can I do? I can move this macro and place it closer to the relevant ports.
If I do this, you’ll notice that the routing will be very minimal, and the spacing will be efficient. Always try to place a macro closer to the port or component with which it is connected or communicating. By doing so, you can reduce the routing length, which helps minimize RC (resistance-capacitance) effects and improves timing. Short routing distances between the pins of your macros and standard cells are crucial. Ensure that all macro cells have access to power and ground nets while maintaining acceptable IR drop. Every cell will have VDD (power) and VSS (ground) pins, and none of these pins should be floating. Place your cells in such a way that they are properly connected to both VDD and VSS pins. Finally, maximize the available area for placing standard cells by creating a contiguous core area. This gives the tool ample space to work with during placement, resulting in an optimized layout.
Sir, is the goal of floorplanning clear? Okay, moving on to power planning. As I mentioned earlier, power planning is the process of adding metal wires to connect your VDD and VSS (power and ground) rails to all the macro cells and standard cells, linking them to the external power supply. The key objective is to keep IR drop (voltage drop due to resistance) under control. When creating the power grid, you need to experiment with different parameters, such as the width of the metal routes, the spacing between the metal routes, and the number of power nets (VDD and VSS straps) you need to create. All of these variables will affect the IR drop. For example, if you vary the width of the metal routes, the IR drop will change. Similarly, if you vary the spacing between the routes or the number of VDD and VSS straps, the IR drop will also vary.
Each of these three parameters, when varied, will result in a different IR drop. There will be a target IR drop specified. For example, if your supply voltage is 3 volts and a 5% drop is allowed, what does that mean? Five percent of the supply voltage is allowed as a drop. So, 5 out of 100 multiplied by 3 equals 0.15. This means a drop of 0.15 volts is allowed. Therefore, if your supply voltage is 3 volts, you can have a drop until 2.85 volts (3 - 0.15 = 2.85). If the drop exceeds 2.85 volts, you need to adjust factors like the width, spacing, or the number of nets. Got it? Did you understand, everyone? What do you mean by "IR drop"? Please respond with "yes" or "no."
Others, there are only a few "yes" responses. What about the rest? Did you get an idea of what "IR drop" means? Okay, so this is how you’ll approach it. In the tool, the tool will create the power mesh or power structure, but for the tool to work effectively, you need to provide certain inputs. You have to experiment with the width value, the spacing value, and the number of nets. Only then will the tool generate an optimized IR drop map for you. Additionally, the tool must meet the EM (Electromigration) constraints, which are related to DFM (Design for Manufacturing) issues. As I mentioned earlier, regarding DFMs, if you analyze your design, any long nets can cause problems. For example, if your core area is structured this way, and the longest net running through the core looks like this, then this longest net tends to have...
Your EM (Electromigration) issues arise due to electron migration. As the length of a net increases, the likelihood of EM issues becomes greater on that net. When the tool creates a power strap, it also considers EM constraints. Even if you create a long power net, the tool will evaluate whether it has any EM issues, as these constraints are already built into the algorithms. Additionally, the tool checks for IR drop to ensure it stays within the target limits. If all possible options fail to meet the requirements, the tool will generate a power strap but will also provide a map indicating areas where there is significant variation in IR drop.

Ma’am, how is the IR drop set? Is there any constraint for it? Yes, there are constraints. The top-level design engineer determines the acceptable IR drop, which is typically expressed as a percentage of the supply voltage. For example, it could be set to 5%, 2%, or 3% of the supply voltage, depending on the design requirements and specifications.
That is given as a constraint to each block engineer, so the block engineer has to target it for that drop. Clear? Got it. Yes, ma’am. All of this comes from experience. Even the top-level engineer, who decides whether the constraint should be 2%, 3%, or something else, does not make this decision randomly. It is based on experience. For example, if they set a target of 5% for the block level, when the design is merged at the top level, the violations might still increase or worsen. Therefore, they usually give a very tight constraint to the block level, leaving some relaxation when merging it to the top level. If the block-level engineer meets the constraint, the top-level engineer can handle small violations because they have been given a tighter margin for the block level. All of this is developed through experience.
Ma’am, your voice is breaking—you’re not able to hear me. Is your voice breaking now? It’s still breaking intermittently. Let me know if the voice keeps breaking continuously.  

Yes, so this is about power planning. Here, there are some peaks that give us information about what power planning entails. If you look at this final product that I have, this is my final product. Now, if you examine this particular product closely, you’ll see the VSS pin and the VDD pin. If you zoom in on this specific portion of the IC, this is how it looks inside the IC. Again, if you zoom in further into this particular area, these are the IO cells that are connected. If you zoom in even closer, this is the zoomed-in portion of the IO cells. Here, you can see the VDD and VSS straps that are running...
Next to your I/O cells, the I/O cells will take power from here—the VDD and VSS tracks. If you look at this portion, the yellow portion of your IC (Integrated Circuit), this is a zoomed-in view of the center area. These are the standard cells. Again, if you zoom in further in this area, you’ll see the VDD and VSS straps. Here, this is one VDD strap, this is another VDD strap, and this is one VSS strap. Again, VDD, and again VSS—these are the straps that are present. All these are the VDD and VSS interconnection straps for the standard cells. These are the I/O cells, and these are the macros. This is your macro here, and the macro connections are here. If you look closely, these are the macros, and the macro...
These are the pins of the macros, and you can see here that there are VDD and VSS supply pins for the macro. This might be a VDD pin over here, and this might be a VSS pin—I’m not entirely sure, but this is how the connections are made. Is that clear, ma’am? Yes, here we don’t give a continuous space for standard cells. What happens if I don’t provide a continuous space for standard cells? Ma’am, if you don’t give a continuous space for standard cells, you’ll likely encounter timing violations. I’ll explain why. Preeti, you’ve put a query in the chat window—I’ll address that next. I didn’t understand your query, okay? So now, this is my continuous core area.
Say I place a macro here, one macro here, another macro here, one macro here, and here. Now, you can see that there is some space here and some space here. What happens is, if there are standard cells to be placed, and corresponding to this macro, I have a standard cell sitting here, the available space might not be enough for all the standard cells. Related to this macro, there will always be a bunch of standard cells, and related to a group of macros, there will be a cluster of standard cells. When the tool tries to place the standard cells, it will attempt to place them near the corresponding macro. This is because the interconnections will be minimized, as the standard cells will primarily communicate with those specific macros.  

Now, corresponding to this macro, the leftover cells get placed here. If there is a connection to be...
Established—if there is a connection to be established, it has to be made this way. As a result, the routing length increases between the two cells. Similarly, for all these cells, the same applies. If the routing length increases, the RNC (Resistance, Capacitance, Inductance) values also increase because routing inherently introduces RNC. If the RNC values increase, the time delay of the signal net increases, which in turn reduces the target frequency of operation. On the other hand, if you had created a **contiguous core area** and placed all the macros in this manner, then all the standard cells related to those macros would be situated here itself. Almost all the cells would be located close together in this area. If you observe closely, you’ll see...
The interconnections will be very minimal. No, see here—which one is better, the first one or the second one? The second one, right? Yeah, again, this depends on the netlist you’re working with. This is one case, but it may not be the same in every situation. Even if you haven’t created a continuous core area, you might still get the best results. It purely depends on the type of netlist you are working with—whether it has fewer standard cells and whether those standard cells are not heavily interconnected. Very few of them might be interconnected, so it is entirely dependent on the netlist. This is why we try to place the macros on the periphery, so that you have a contiguous core area. This reduces the routing length between the standard cells and between the macros by a larger amount. Hope it is clear for everyone? Yeah, ma’am. Ma’am… mhmm. Should we not connect…
There will be a connection between macro cells and standard cells, ma’am. Yes, there will be a connection, and it purely depends on the netlist, ma’am. Okay. Are I/O ports also connected to macros, ma’am? Yes, I/O ports will be connected to macros, and they will also be connected to standard cells, ma’am. Okay, ma’am. For one macro, how many I/O cells are connected, ma’am? For one macro, the number of I/O cells connected depends on your netlist, ma’am. It depends on the type of RTL code you write. For example, clock pins, reset pins, and set pins are common, but you cannot place every cell next to the clock pin because it is shared by all the blocks. There are certain pins that...
Specific to a group of cells, place those cells near the specific input pins, output pins, clock pins, reset pins, or set pins. You can neglect some pins, such as clock pins, reset pins, or set pins, because they are connected to almost all the cells, macros, or standard cells. Wherever you place them, the routing length may increase, but that is acceptable. They will provide you with a flowchart, diagram, or document detailing how the blocks connect to each other. You need to study this document and place the macros accordingly. You cannot place the macros arbitrarily; their placement depends on the RTL design. Ma'am, yes, because the number of inputs and outputs comes from the RTL code. The design specifies how many inputs and outputs you have.
You’ve explained it clearly—everyone understands! So, if you look at a chip-level view, the power routes are not clearly visible. The green lines here represent the power grids (not the blue ones). These green lines are the power grids, both vertical and horizontal. The red lines also represent the power grids. These are the macros, and these are the I/O cells. Your standard cells are located here. What is written in blue? This is one demonstration of how power is distributed at the chip level. If you consider a block-level view, this is how the power distribution looks. The purple line and the light...
These are the power and ground grids, so you create a power grid in both the vertical and horizontal directions. This is because we don’t know which pins of your macros and standard cells will tap power from the vertical straps or the horizontal straps. Therefore, you create both types of straps—one horizontal strap and one vertical strap.  

These are the ports, but they are not input/output ports. They are ports with respect to the block level. In the context of the chip level, these would be called “pins,” but at the block level, we refer to them as “ports.”  

This is how we handle power routing. You don’t manually create these power routes. Instead, there is a command that generates them, but you need to provide the necessary inputs to the command, such as the width, spacing, pitch value, and other parameters.
The value you provide as input determines how well the tool creates these power straps. Once this is done, you can generate an IR drop map. This is about the IR drop analysis. Once the power nets are laid out or created, you can distribute power to all the cells in the design. The cells will draw current, and as a result, you will have an IR drop. For example, if you have a VDD supply of 1.2 volts, the total resistance of your power net is 12 ohms, and the current drawn by individual cells in the design is 100 milliamps, then what will be your IR drop? The IR drop is calculated as \( R \times I \), which is \( 12 \times 100 = 120 \) millivolts. So, the drop will be 120 millivolts. If you consider 120 millivolts, this is nearly equal to 0.12 volts.
Two volts minus one point two volts equals point zero eight. So, one point zero eight is the allowed voltage drop. This one point zero eight voltage drop corresponds to a certain percentage. Typically, this is limited to five percent of the supply voltage. If the supply voltage is one volt, five percent of it would be calculated as five divided by one hundred, which is zero point zero five volts. Subtracting zero point zero five from one volt gives you zero point nine five.
A 0.95-volt drop is allowed. If you get 1 volt, it is acceptable. If you get 0.95 volts, it is also acceptable. However, if the drop is less than that, it is not good, and you need to reframe your power plan in that case. Preeti, you had a query, right? You sent it in the chat window. What was that, Preeti? Preeti Rajkopal, I didn’t understand your query—are you there, Preeti? Okay, it’s clarified. Fine. So, this is how you calculate the IR drop. The tool calculates the IR drop based on the target you set. When you set the target, the tool builds the IR drop map and tries to meet the...
But sometimes, the tool may not be able to meet the target IR drop. If it fails to meet the target, it will calculate the actual IR drop for the power grid and display it as a map. From this map, you can gather information about the IR drop. Here is the map that the tool generates. If you look closely, you’ll see a color-coded representation: the red region indicates where the IR drop is highest. How much is the IR drop? It is specified on the left-hand side, where you see "42.1 millivolts." If your target IR drop is, say, 25 millivolts (meaning 25 millivolts of drop is allowed), but the tool shows that the actual drop is 42.1 millivolts, this region appears in red. The next color code is light orange, indicating an IR drop of around 37 millivolts, and the yellow region shows an IR drop of around 33 millivolts. This means the tool is highlighting areas with higher IR drop in the power grid.
If the IR drop is higher than the target, you need to go back and adjust your power planning strategy. Recreate the IR drop map again, and repeat this process until the IR drop is within the target limit—specifically, it should not exceed 25 millivolts; ideally, it should be less than 25 millivolts. These are the terminals where the power straps tap the VDD and VSS points. As you move toward the center of the design, you’ll notice that the IR drop increases, which is why the red region appears there. When you analyze this IR drop map, you’ll gain insight into where the IR drop occurs and what changes are needed to address it. This is your IR drop map.

A good power route is one where the IR drop is within the target limit, and there are no DRC (Design Rule Check) or LVS (Layout Versus Schematic) violations. After you create the power plan, you must check for these violations to ensure the design meets all requirements.
See, there should be minimal routing resource consumption, meaning there should be enough space for signals to be routed without lag. Of course, you can achieve better IR drop by creating more routes. If your core area is filled with power straps, it won't leave enough room for signal routing. If your design is full of power straps, there won't be sufficient space for signals to be routed. Again, signal routing must be done correctly. Creating too many power straps in this manner results in a poor power plan. Instead, you should aim for a minimal power plan, where the power routes are kept to a minimum. This way, you can achieve a relatively good power plan.
As compared to this one, you can see here that it is filled with power routes only, leaving no spacing for signal routing. You need space for clock routing and signal routing, so this one is a better power plan. If you consider this one as a good power plan and the other one as a bad power plan, that is the meaning of having minimal routing resources. Got it? Next, let’s talk about standard cell placement. The process of automatically placing your standard cells in the created rows is called **standard cell placement**. Once you create the power grid, the remaining standard cells will be placed automatically by the tool. Again, this placement of standard cells is an iterative process because, when you analyze the QR (Quality of Results) reports, if the results do not meet your specifications, you have to redo the placement. So, the impact of timing is...
Also, several factors are considered for a given placement before it is finalized. Your automatic placement result is not finalized in just one run. It might take around twenty placements, twenty iterations, or even fifty runs—or it might take as few as five runs. This entirely depends on how well you place your macros and how effectively you perform your power planning.  

This is your placement flow: Initially, you create a floor plan. The input floor plan is already created, and the power plan is also prepared. Then, from the original netlist, you detach your scan chains because the scan chains are attached during the synthesis stage. After detaching the scan chains, you place your standard cells. Once the standard cells are placed, you reconnect the scan chains, which is relatively straightforward since the tool will automatically handle the reconnection. Finally, the tool performs a condition check. This check ensures there is enough space for the clock nets and signal nets to be routed, as the power routing has already been completed. If the power routing...
If the design has taken up more space or consumed more routing resources, you'll encounter more issues. The condition level should ideally be zero. If your condition level is zero, you can proceed to check the timing. However, if the condition level is not zero, you need to go back to the floor plan and power plan stages and make the necessary changes. You cannot proceed further with the design because, at the routing stage, it will result in errors—such as floating nets and pins. Once the timing is verified and found to be okay, you move to the CTS (Clock Tree Synthesis) stage. If the timing is not satisfactory, you apply additional constraints for timing optimization. After connecting the scan chains, you can extract the scan chain information in the form of a DFT (Design for Test) file. So, this is about the detachment of scan chains. Ma'am, yes, that is done manually.
Yes, we have to use a command. You need to give the command `remote_scan_chain` in the ICC tool. The command is `remote_scan_chain`. Okay, okay. So, this is how your design looks after you place your standard cells. If you look here, this is the design after your power plan. Here, only the macros are placed, and the power routing is done, but the standard cells are still not placed. However, after you complete the placement, this is how the block looks. The standard cells are placed in the center, and you can see there is a contiguous core area for the tool to place your standard cells. Whatever is shown in this pink color is just...
If you zoom in, this is how your standard cells look. Here, you can see instances of your flip-flops. `sdff.dot_frm` is a view, specifically a frame view, which is one type of layout view. Note that this is not the complete layout view; it is an extracted view. The extracted view is in the `.frm` format, which is one of the formats used for layout representation. Almost everything in this view is related to the D flip-flops themselves. The small blocks here might be combinational logic blocks as well.  

You can see one VDD strap running here, and for these standard cells, this is your continuous VDD and VSS strap. There is one VDD and one VSS, one VDD and one VSS, and so on. For the standard cell, if this is the VDD pin, it will be connected here.
The VSS pin will be connected here. Again, for this standard cell, this is your VSS, and this will be your VDD. Clear?  

So, what makes a good placement? It must be possible to route the design. At the condition level, there should be zero violations—it should not run out of area to connect the nets. This is nothing but ensuring that the condition level is zero. It should also meet the timing goals with respect to setup and hold. Usually, at the placement stage, we check only for setup and hold violations. We check for these at the CTS (Clock Tree Synthesis) stage as well. Additionally, it should have no DRC (Design Rule Check) or LVS (Layout Versus Schematic) violations.  

Next is the CTS (Clock Tree Synthesis) stage, where you route your clock nets. For example, if you have one clock port and four flip-flops that need to be connected, prior to CTS, each flip-flop’s clock pin will be directly connected to the clock port. So, if you look here, the clock pin over here will have four fan-outs—one for each flip-flop.
One, two, three, four. Because of these four fanouts, you have four loads connected. As a result, the transition might be very high, causing the slew rate to be very high. Due to this, there will be delay in the gate because the gate delay depends on two factors: the input slew and the output load. When you have more fanouts, transition violations may occur, leading to delays in the signal. Consequently, the delay at the output of this D flip-flop will be higher, and the skew at the output pins of the flip-flop will experience larger delays.  

In the CTS (Clock Tree Synthesis) stage, the tool will attempt to buffer these three clock nets. It will insert buffers in such a way that each buffer handles only two outputs. For example, if you look here, Buffer 1 has only two output descendants. Similarly, Buffer 2 also has only two output descendants.
Similarly, this third buffer also has only two connections, so the load is equally distributed. Right? Now, the load is shared. This is nothing but building your clock tree. For this, you have to set a target to the tool. If the fan-out is, say, ten, then the tool will insert drop buffers, add buffers, or inverters (whichever is appropriate) to distribute the load. Usually, the tool builds a clock tree to minimize skew. We will discuss the concept of skew in the S-tier tomorrow, ma'am.  

However, if you use buffers, the area will increase. No, ma'am, yes, of course. But if your timing is not met, what will you do with the functionality? Okay, ma'am. But here, instead of using three buffers, if we use two clock pins, then the area will be reduced, and the processing speed will also be reduced. What do you mean by "two clock pins"? Are you saying we have to use two oscillators?
Yes, creating an oscillator circuit is indeed a very complex task because it is an analog circuit, and it comes with its own set of challenges. Inserting a buffer is far better than creating two clock pins—got it, ma’am? Of course. Here, you might achieve the functionality, but the timing may not be met. Sometimes, due to transition violations, even the functionality may not be achieved. If your design itself is not working, what good is saving space? If the functionality is not met or the timing is not satisfied, what benefit do you gain from saving space? It is worth sacrificing some space to ensure that the timing is met, even if the area increases slightly. That is...
You have to compromise because, when you go for VLSI (Very Large Scale Integration) design, you’ll be concentrating on three key domains or terms we call it as **PPA**: **Power**, **Performance**, and **Area**. Power means we try to reduce the power consumption. Performance means we try to increase the speed. Area means you try to reduce the size of the chip. You cannot get all three at their best simultaneously—that is, you cannot achieve a reduced area, the best possible speed, and the lowest power consumption all at once. You have to sacrifice one of them. This is the concept behind balancing PPA trade-offs.  

Now, let’s see what we mean by **clock skew**. We were talking about clock skew, right? So, consider your entire chip. You have these two blocks: a D flip-flop labeled as **D Flip-Flop 1** and another labeled as **D Flip-Flop 2**. The clock reaches **D Flip-Flop 1** at 1 nanosecond, and the clock reaches **D Flip-Flop 2** at 1.2 nanoseconds. If you look at the difference in the clock arrival times at the pins of these D flip-flops, what is...
The difference is 1.2 minus 1, which is 0.2. So, the clock skew is 0.2 nanoseconds. When the tool builds a clock tree, it tries to construct the clock tree in such a way that the clock skew is minimized, ideally aiming for zero clock skew. The tool attempts to reduce the clock skew so that all the flip-flops receive the clock signal at the same instant. However, achieving zero clock skew is practically impossible. You will always have a minor clock skew, but as long as it does not affect your setup and hold times, it is acceptable. Having a minimal clock skew is feasible. 

Now, why do you get clock skew? The primary reason for clock skew is the wire length. Consider this clock pin: from here to here, there is a wire length for the first connection. For the second connection, the wire length is longer. As the wire length increases, the RC (Resistance-Capacitance) delay also increases. As the RC delay increases, the signal propagation time changes, leading to clock skew.
At every instant, this entire process involves the charging and discharging of the RC (resistance-capacitance) network, causing the RC value to change. As a result, the signals will reach the clock pins of all the flip-flops in your circuit at different intervals. Here, you have shown just two flip-flops, but in a real design, you might have around ten thousand, one hundred thousand, or even one lakh D flip-flops, depending on your design. This variation leads to clock skew.  

Is that clear? What do you mean by "clock skew"? Clock skew refers to the difference in delays of the clock signals between two sequential cells.  

What do you mean by "positive clock skew"? If you look at the slack here, this is your Flip-Flop 1 and Flip-Flop 2. The clock pin is over here. The data that is...
At the data input (deep end) of this flip-flop, the arrival time (AT) is measured from the clock port to this point. This is the time it takes for the signal to arrive at the flip-flop. However, the required time (RT) for the flip-flop is different. The required time represents the latest moment the signal must arrive at the flip-flop to meet the setup or hold requirements. Here, the AT is coming outside the RT, meaning the signal arrives later than it should. 

When you calculate the setup slack, many people may not fully understand what "setup slack" means, or what "AT" (Arrival Time) and "RT" (Required Time) represent. Are you aware of these terms? No, ma’am? No, ma’am? Okay, I’ll give you a brief explanation of what AT, RT, and setup slack mean.
Set-up slack. So, when you do an STA (Static Timing Analysis) check, the tool will always consider the timing between any two flip-flops in the entire chip. It will consider any two flip-flops that are connected via a path. At every pair of flip-flops, it will perform a timing check. Here, if I consider **Flip-Flop 1** and **Flip-Flop 2**, the data arrives at the **D** pin of Flip-Flop 2, but until the clock arrives, the data at the **D** pin won’t be available at the **Q** pin. Therefore, when you consider the arrival time, you’ll always be looking at the timing relationship between the clock and the **Q** output.  

If you consider the arrival time of the data, 80 ns (for example) is the arrival time between these two flip-flops. The arrival time is essentially the **clock-to-Q delay** of Flip-Flop 1, plus any combinational delay between the two flip-flops. Here, there is no...
It is only a wire that connects the combinational block, so for now, I am neglecting the delay caused by this interconnecting wire. Let’s assume the delay of the interconnecting wire is zero. So, for the time being, I am ignoring the interconnecting wire delay. This is nothing but the clock-to-queue delay. Whatever queue is present, it becomes instantly available at this point. This is my arrival time.  

Now, what is my required time (`RT`)? `RT` stands for the required time. The required time for setup is nothing but the time period of the clock. This is one time period. The time period of the clock is represented as `P`. Subtract the setup time of the flip-flop. Each flip-flop has its own setup time, so we subtract that setup time. This becomes your required time. This is my required time (`RT`). The `RT` mentioned here is incorrect. This is my correct `RT`.  

Now, when I calculate the setup slack, the setup slack equation is...
It is equal to RT (Required Time) minus AT (Arrival Time). When the tool checks the setup slack, the setup slack value should be zero or a positive value. If it is negative, it indicates a violation. Now, looking at the equation RT – AT, when will there be a violation? When will this value become negative? This happens when AT (Arrival Time) is larger than RT (Required Time). In other words, if the arrival time exceeds the required time, you have a violation. What does it mean if the arrival time is larger? This refers to the clock-to-queue delay. If this delay is very large, it leads to a violation, which we call a setup violation. To compensate for this setup time violation, what do you do? You add a delay to the clock path. When you add...
The delay in this clock path gets added to the `RT` (required time), which increases the `RT`. If I keep `RT` as 80 (not changing it), and I increase `RT` by adding this delay, I am essentially trying to make the negative value (setup slack) become zero or positive. This is nothing but introducing a **positive clock skew**. By adding a delay to the clock path, you are fixing the setup violation. This is called a **positive clock skew**.  

Do you understand this? Any questions?  

Here, you can see that after adding the delay, the `AT` (arrival time) shifts inside the `RT`. Before, `AT` was outside `RT`, but now `AT` has shifted, and it is smaller than `RT`. As a result, there is no setup violation anymore.
Understanding setup and hold is extremely important, not just for RTL designers, but also for FC (Floor Planning) and PD (Physical Design) engineers. Throughout your career, you will constantly deal with setup and hold constraints. For ATL (Analog Layout) and FC engineers, these concepts are fundamental, and they will likely ask you about them during interviews. Questions such as "What do you mean by setup time of a flip-flop?" or "What do you mean by hold time of a flip-flop?" or "What is setup slack?" or "What is hold slack?" are common. You should have a strong grasp of these concepts.  

We will cover this topic again in tomorrow's session, but try to understand the basics: what is setup time, what is hold time, and what is slack. The same principle applies here—we'll place a delay element after the buffer.
A delay element—this delay element could be a buffer. In the previous slide, we discussed how a buffer can help reduce skew. However, this buffer is **not** making the skew zero. The skew already exists, so if you want to reduce it, you need to add buffers at the appropriate places. You can add a buffer here or to the `FF1` path as well. Okay? If you look at the next slide, this is your whole slack. When you calculate the whole slack, it is...
It is 80 ns minus RT. Both 80 ns and RT are the same in the context of calculating the whole slack. When you calculate the whole slack, it is 80 ns minus RT. For **setup slack**, it is RT minus 80 ns.  

Now, regarding **hold slack**: When do you say there is a violation? When the value is negative. If it is zero or positive, there is no violation. When do you get a negative number? When RT is a larger value. What is RT? RT is nothing but the **hold time** of Flip-Flop 2. So, if RT is a larger value, you get a violation.  

What do you do in this case? You add a delay here. By adding a delay, you effectively increase RT. Initially, if you see here, RT was smaller than 80 ns, so there were no violations. However, since you added a buffer to overcome the setup violation, this has resulted in a hold violation.
So now you have a hold violation here because your RT (Required Time) has increased. This delay gets added to the RT, which causes your hold violations to worsen. If you add a positive clock skew to improve your setup violation, your hold might degrade as a result. When the tool performs calculations and decides where to add buffers, it must consider both the setup slack and the whole slack. It cannot add a buffer to resolve a setup violation if it introduces a new hold violation. In such cases, the tool must explore alternative solutions. These advanced fixes will be discussed in the advanced STA (Static Timing Analysis) concepts. This is how the tool tries to optimize your design.
So, this is the structure of your clock tree. Once the clock tree is built, if you look at its structure, this is how it appears. This is my clock net here—the main clock net. From here, it is routed to the center, and from there, you can see these are the clock pins of the macros. All of these are clock pins, and in between, there are many standard cells, each with many clock pins. It’s difficult to identify exactly which route goes where in the standard cells, but in the macros, of course, you can see the clock pins clearly. These are the clock pins.

So, once the CTS (Clock Tree Synthesis) is built, this is how the block looks. In the CTS stage, after placement, you run the CTS flow. The tool will try to add buffers and inverters to reduce your clock skew. This process is called building the CTS. Additionally, while building the CTS, the tool also tries to...
Timing optimization aims to reduce setup and hold violations. Once your clock tree is built, it will perform the routing of that particular clock tree. Remember, at this stage, **only the clock tree routing is done**—no signal routing is performed yet.  

So, what makes a good clock tree? The clock skew should be minimized. Ideally, it should be zero, but practically, achieving zero skew is not possible. A target should be set for the additional timing violation caused by clock skew, ensuring that it can be fixed with minimal effort. As I mentioned earlier, if adding a buffer to fix a setup violation results in a hold violation, this new violation should be fixable with minimal effort. Of course, there should be no DRC (Design Rule Check) or NPS (Netlist vs. Schematic) violations.  

The final stage is signal routing, where the remaining signal nets are routed. This is how the structure looks: the clock routing is completed, and the remaining signal nets need to be routed. The tool will perform the signal routing by adding the necessary metal layers and connections.
Sender wires form the base transistor structure, upon which the tool builds horizontal and vertical metal steps by adding the corresponding wires. During routing, each and every pin of your standard cell should be connected, ensuring there are no open nets after the routing is completed. To facilitate automatic routing, the metal layers used by the tools for routing are oriented in horizontal and vertical directions. Metal 1 will be horizontal, Metal 2 will be vertical, Metal 3 will be horizontal again, and Metal 4 will be vertical. This fixed direction of the metal routes is established to enable more efficient and compact routing. The direction of the metal routes is defined in the technology file, which is provided by the foundry. The foundry specifies which metal rail layers are routed in which direction.
The routing flow is done after the CTS (Clock Tree Synthesis) stage. The output of CTS is given as input to the routing stage. In the CTS stage, you have already completed the routing of the clock nets. In the routing stage, you will perform the routing of the signal nets. Once the signal nets are routed, you will conduct **post-route timing optimization**. This is necessary because, while the tool is performing the routing, some of the sequential cells might be moved slightly, which needs to be corrected. The tool performs post-route timing optimization to fix these issues.  

Once the routing is complete, you need to extract the parasitic information because the final routing has been done. Extracting the parasitic information allows you to perform **back annotation**, and then you conduct the STA (Static Timing Analysis) check again. This is referred to as the **post-layout STA check** that you will perform. So...
The router design looks like this, with all the metal routes highlighted in different colors. Here, you can see the BSS (backside supply) and VDD nets, which are the power routes. The remaining nets are the signal routes.  

Now, what makes a good routing solution? It should use the shortest possible wire length, as this reduces the RC (resistance-capacitance) values. Additionally, it should cause minimal disturbance to the timing, since by this stage, you should have already fixed most setup and hold violations. However, due to routing, there may still be small timing violations, but these must be kept to a minimum. Furthermore, there should be no DRC (Design Rule Check) or LVS (Layout Versus Schematic) violations.  

What are the responsibilities of an APR (Automatic Place and Route) engineer? An APR engineer, who is essentially a PD (Physical Design) engineer, must deliver a layout that meets the following criteria:  
- It should have no IR drop issues.  
- It should be free of DRC, LVS, and DFM (Design for Manufacturability) issues.  
- It should have zero timing violations.
And it should have zero LEC (Logical Equivalence Checking) errors. LEC is essentially about checking for functional correctness. So, what are the deliverables of an API (Advanced Physical Design) engineer? The API engineer should provide you with a layout in GDSII format because this is the format specified by the foundry. The layout should not have any DRC (Design Rule Check) or LVS (Layout Versus Schematic) violations. There should also be no setup and hold violations. The gate-level netlist should include a clock tree, scan chain information, and timing information. Additionally, they should provide the reports related to LEC checks and DRC checks.

When the design review team reviews the design, they will analyze it based on the timing reports. They won’t understand anything by simply looking at the layout; instead, they review the design using the timing reports, power drop reports, DRC/LVS check reports, LEC reports, and other relevant data.
These are some of the tools I use for the APR (Application Place and Route) flow. In the APR flow, Magma provides the Olympus tool, Synopsis offers ICC, and Cadence provides Silicon Ensemble. Typically, when considering a top-level design or a block-level design with a large number of cells (e.g., a design with two million gates of logic), the runtime is usually around one day. In the PD (Power Distribution) flow, we work with a netlist that has around 38,000 to 40,000 gate counts, which takes approximately 45 minutes to one hour to run. However, if you consider a netlist with two million gate counts, it usually takes one full day to run in the APR flow.  

That’s why, when you initiate a particular run, you should be very careful. You have...
To ensure that all setup files are properly given, all parts are specified, and all constraints are correctly loaded, we must provide all the necessary inputs for the tool to produce the best possible results. If, somewhere in the process, you realize that you’ve missed out on one of the constraints, you’ve already wasted a significant amount of runtime, and fixing it afterward becomes a major challenge. This API flow isn’t completed in one stretch; you might need to iterate multiple times. So, that completes the PD (Physical Design) flow. Any doubts? And what is a netlist? Arvind, after all this discussion, can anyone answer this question? Anyone of you?
The output of synthesis is the one we will be dealing with throughout the entire API (After Place-and-Route) flow. Here, we are in the RTL (Register-Transfer Level) design stage. In RTL design, you write an RTL code, create a test bench for verification, and then pass it through synthesis. The output of synthesis is a **netlist**, which is essentially a file containing information about the interconnections of gates. The RTL code is converted into a gate-level netlist—a text-based format that includes instances of your library cells. It will also have...
All gates—such as NAND gates, NOR gates, flip-flops, and others—will be included in the netlist, along with the interconnect information. For example, the input of one flip-flop should be connected to the output of a gate, and all such connection details will be present in the netlist. So, the netlist is a textual file derived from your RTL code through a process called synthesis. Is this clear for you?  

Yes, ma’am. Thank you. From the synthesis stage onward, we will be working purely on the netlist itself. We are no longer working directly with the RTL code.  

Any other questions, ma’am? Regarding scripting—where will it happen? Whether it will happen in the RTL stage or after the RTL stage? You’re asking about RTL scripting. Yes, RTL scripting is done during the RTL design phase in the APF (After Place-and-Route Flow) flow.  

Uh-huh. We are currently setting up the design. So, in the APF...
